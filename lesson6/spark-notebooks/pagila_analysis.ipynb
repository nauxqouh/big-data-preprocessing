{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Lesson 6. Pagila Database Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements.\n",
    "\n",
    "1. Using Spark, compute monthly revenue by film category.\n",
    "2. Define customer lifetime value (CLV) using Spark.\n",
    "3. Identify the top 1% of customers generating 80% of revenue.\n",
    "4. Propose a partitioning strategy for the payment table:\n",
    "    - by date?\n",
    "    - by store?\n",
    "    - by customer?\n",
    "    \n",
    "    Explain trade-offs.\n",
    "\n",
    "5. The following join is very slow at scale:\n",
    "\n",
    "    `payment -> rental -> inventory -> film -> film_category -> category`\n",
    "\n",
    "    Propose:\n",
    "\n",
    "    - join order optimization\n",
    "    - indexing strategies\n",
    "    - caching or materialized views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Spark Session and JDBC Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "25/12/30 10:35:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum as _sum, count, avg, round as _round, max as _max, min as _min, \\\n",
    "    stddev, percentile_approx, \\\n",
    "    date_format, datediff,  \\\n",
    "    countDistinct, concat_ws, \\\n",
    "    row_number, broadcast\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PagilaAnalysis\") \\\n",
    "    .config(\"spark.jars\", \"../jars/postgresql-42.7.8.jar\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"../jars/postgresql-42.7.8.jar\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "jdbc_url = \"jdbc:postgresql://localhost:5432/analytics\"\n",
    "db_properties = {\n",
    "    \"user\": \"spark_user\",\n",
    "    \"password\": \"spark_password\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n=== READING PAGILA DATA FROM POSTGRESQL ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n=== All tables loaded! ===\n"
     ]
    }
   ],
   "source": [
    "print(\"\\\\n=== READING PAGILA DATA FROM POSTGRESQL ===\")\n",
    "\n",
    "# Read all tables needed\n",
    "df_film = spark.read.jdbc(url=jdbc_url, table=\"film\", properties=db_properties)\n",
    "df_film_category = spark.read.jdbc(url=jdbc_url, table=\"film_category\", properties=db_properties)\n",
    "df_category = spark.read.jdbc(url=jdbc_url, table=\"category\", properties=db_properties)\n",
    "df_inventory = spark.read.jdbc(url=jdbc_url, table=\"inventory\", properties=db_properties)\n",
    "df_rental = spark.read.jdbc(url=jdbc_url, table=\"rental\", properties=db_properties)\n",
    "df_payment = spark.read.jdbc(url=jdbc_url, table=\"payment\", properties=db_properties)\n",
    "df_store = spark.read.jdbc(url=jdbc_url, table=\"store\", properties=db_properties)\n",
    "df_customer = spark.read.jdbc(url=jdbc_url, table=\"customer\", properties=db_properties)\n",
    "\n",
    "print(\"\\\\n=== All tables loaded! ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1. Monthly Revenue by Film Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n=== EXERCISE 1. Monthly Revenue by Film Category\n",
      "\\nMonthly Revenue by Category:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------+\n",
      "|year_month|category_name|total_revenue|\n",
      "+----------+-------------+-------------+\n",
      "|2022-01   |Sci-Fi       |244.43       |\n",
      "|2022-01   |Action       |237.44       |\n",
      "|2022-01   |New          |236.47       |\n",
      "|2022-01   |Sports       |235.48       |\n",
      "|2022-01   |Comedy       |226.57       |\n",
      "|2022-01   |Drama        |218.55       |\n",
      "|2022-01   |Foreign      |210.48       |\n",
      "|2022-01   |Documentary  |195.51       |\n",
      "|2022-01   |Family       |189.53       |\n",
      "|2022-01   |Classics     |168.58       |\n",
      "|2022-01   |Animation    |164.54       |\n",
      "|2022-01   |Travel       |150.68       |\n",
      "|2022-01   |Horror       |147.68       |\n",
      "|2022-01   |Music        |136.63       |\n",
      "|2022-01   |Games        |129.71       |\n",
      "|2022-01   |Children     |123.68       |\n",
      "|2022-02   |Sports       |821.20       |\n",
      "|2022-02   |Games        |728.34       |\n",
      "|2022-02   |Family       |709.12       |\n",
      "|2022-02   |Action       |684.34       |\n",
      "+----------+-------------+-------------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n Pivot View (Month x Category):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 88:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+-------+-------+-------+-------+-------+-------+\n",
      "|category_name|2022-01|2022-02|2022-03|2022-04|2022-05|2022-06|2022-07|\n",
      "+-------------+-------+-------+-------+-------+-------+-------+-------+\n",
      "|Action       |237.44 |684.34 |839.90 |652.34 |628.36 |741.06 |592.41 |\n",
      "|Animation    |164.54 |623.48 |802.02 |779.04 |882.86 |757.02 |647.34 |\n",
      "|Children     |123.68 |520.76 |620.36 |698.27 |611.35 |636.28 |444.85 |\n",
      "|Classics     |168.58 |541.59 |604.43 |579.46 |643.37 |621.44 |480.72 |\n",
      "|Comedy       |226.57 |640.66 |736.31 |633.57 |749.44 |730.42 |666.61 |\n",
      "|Documentary  |195.51 |585.46 |756.24 |646.48 |697.38 |752.02 |584.43 |\n",
      "|Drama        |218.55 |678.36 |768.24 |748.39 |809.11 |720.35 |644.39 |\n",
      "|Family       |189.53 |709.12 |684.28 |744.12 |657.30 |688.20 |553.52 |\n",
      "|Foreign      |210.48 |663.48 |736.24 |623.40 |756.20 |609.50 |671.37 |\n",
      "|Games        |129.71 |728.34 |740.31 |589.64 |753.42 |678.40 |661.51 |\n",
      "|Horror       |147.68 |589.76 |730.41 |561.65 |587.67 |553.71 |551.66 |\n",
      "|Music        |136.63 |521.79 |544.67 |549.67 |588.56 |513.66 |562.74 |\n",
      "|New          |236.47 |626.62 |579.72 |707.49 |718.46 |769.32 |723.49 |\n",
      "|Sci-Fi       |244.43 |658.49 |804.14 |779.31 |796.09 |726.20 |748.32 |\n",
      "|Sports       |235.48 |821.20 |850.08 |861.08 |885.05 |888.04 |773.28 |\n",
      "|Travel       |150.68 |566.57 |592.54 |594.65 |622.59 |506.84 |515.77 |\n",
      "+-------------+-------+-------+-------+-------+-------+-------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"\\\\n=== EXERCISE 1. Monthly Revenue by Film Category\")\n",
    "\n",
    "monthly_revenue_by_category = df_payment \\\n",
    "    .join(df_rental, \"rental_id\") \\\n",
    "    .join(df_inventory, \"inventory_id\") \\\n",
    "    .join(df_film, \"film_id\") \\\n",
    "    .join(df_film_category, \"film_id\") \\\n",
    "    .join(df_category, \"category_id\") \\\n",
    "    .withColumn(\"year_month\", date_format(col(\"payment_date\"), \"yyyy-MM\")) \\\n",
    "    .groupBy(\"year_month\", col(\"name\").alias(\"category_name\")) \\\n",
    "    .agg(\n",
    "        _sum(\"amount\").alias(\"total_revenue\")\n",
    "    ) \\\n",
    "    .orderBy(\"year_month\", col(\"total_revenue\").desc())\n",
    "\n",
    "# Show result\n",
    "print(\"\\\\nMonthly Revenue by Category:\")\n",
    "monthly_revenue_by_category.show(truncate=False)\n",
    "\n",
    "pivot_revenue = (\n",
    "    monthly_revenue_by_category\n",
    "    .groupBy(\"category_name\")      \n",
    "    .pivot(\"year_month\")           \n",
    "    .agg(_round(_sum(\"total_revenue\"), 2))\n",
    "    .orderBy(\"category_name\")\n",
    ")\n",
    "    \n",
    "print(\"\\\\n Pivot View (Month x Category):\")\n",
    "pivot_revenue.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2. Define Customer Lifetime Value (CLV)\n",
    "\n",
    "In the basic background: `CLV = Total Revenue from Customer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n=== EXERCISE 2. Customer Lifetime Value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n Top 20 Customers by CLV:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+------+-----------+-------------+---------------+-----------+-------------------+\n",
      "|customer_id|customer_name |clv   |total_trans|active_months|avg_trans_value|tenure_days|avg_monthly_revenue|\n",
      "+-----------+--------------+------+-----------+-------------+---------------+-----------+-------------------+\n",
      "|526        |KARL SEAL     |221.55|45         |6            |4.92           |171        |36.93              |\n",
      "|148        |ELEANOR HUNT  |216.54|46         |7            |4.71           |180        |30.93              |\n",
      "|144        |CLARA SHAW    |195.58|42         |7            |4.66           |179        |27.94              |\n",
      "|137        |RHONDA KENNEDY|194.61|39         |7            |4.99           |180        |27.80              |\n",
      "|178        |MARION SNYDER |194.61|39         |7            |4.99           |176        |27.80              |\n",
      "|459        |TOMMY COLLAZO |186.62|38         |7            |4.91           |182        |26.66              |\n",
      "|469        |WESLEY BULL   |177.60|40         |7            |4.44           |171        |25.37              |\n",
      "|468        |TIM CARY      |175.61|39         |7            |4.50           |174        |25.09              |\n",
      "|236        |MARCIA DEAN   |175.58|42         |7            |4.18           |180        |25.08              |\n",
      "|181        |ANA BRADLEY   |174.66|34         |7            |5.14           |183        |24.95              |\n",
      "|176        |JUNE CARROLL  |173.63|37         |7            |4.69           |182        |24.80              |\n",
      "|259        |LENA JENSEN   |170.67|33         |6            |5.17           |163        |28.45              |\n",
      "|50         |DIANE COLLINS |169.65|35         |7            |4.85           |174        |24.24              |\n",
      "|522        |ARNOLD HAVENS |167.67|33         |6            |5.08           |163        |27.95              |\n",
      "|410        |CURTIS IRBY   |167.62|38         |7            |4.41           |179        |23.95              |\n",
      "|403        |MIKE WAY      |166.65|35         |7            |4.76           |176        |23.81              |\n",
      "|295        |DAISY BATES   |162.62|38         |7            |4.28           |181        |23.23              |\n",
      "|209        |TONYA CHAPMAN |161.68|32         |6            |5.05           |167        |26.95              |\n",
      "|373        |LOUIS LEONE   |161.65|35         |7            |4.62           |178        |23.09              |\n",
      "|470        |GORDON ALLARD |160.68|32         |7            |5.02           |179        |22.95              |\n",
      "+-----------+--------------+------+-----------+-------------+---------------+-----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\\n CLV Statistic Summary.\n",
      "+-------+----------+-------+-------+-------------------+----------+-------------------+\n",
      "|avg_clv|stddev_clv|min_clv|max_clv|25th_percentile_clv|median_clv|75th_percentile_clv|\n",
      "+-------+----------+-------+-------+-------------------+----------+-------------------+\n",
      "| 112.55|     25.37|  50.85| 221.55|              94.78|    110.73|             128.71|\n",
      "+-------+----------+-------+-------+-------------------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\\\n=== EXERCISE 2. Customer Lifetime Value\")\n",
    "\n",
    "clv_summary = df_payment \\\n",
    "    .join(df_customer, \"customer_id\") \\\n",
    "    .groupBy(\n",
    "        \"customer_id\",\n",
    "        concat_ws(\" \", col(\"first_name\"), col(\"last_name\")).alias(\"customer_name\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        # Total Revenue (CLV)\n",
    "        _sum(\"amount\").alias(\"clv\"),\n",
    "        # Total Transactions\n",
    "        count(\"payment_id\").alias(\"total_trans\"),\n",
    "        # Number of active months\n",
    "        countDistinct(date_format(col(\"payment_date\"), \"yyyy-MM\")).alias(\"active_months\"),\n",
    "        # Average revenue of each transaction value\n",
    "        _round(avg(\"amount\"), 2).alias(\"avg_trans_value\"),\n",
    "        # Tenure days\n",
    "        datediff(_max(\"payment_date\"), _min(\"payment_date\")).alias(\"tenure_days\"),\n",
    "        # _max(\"payment_date\").alias(\"last_purchase_date\"),\n",
    "        # _min(\"payment_date\").alias(\"first_purchase_date\")\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"avg_monthly_revenue\",\n",
    "        _round(col(\"clv\")/col(\"active_months\"), 2)\n",
    "    ) \\\n",
    "    .orderBy(col(\"clv\").desc())\n",
    "\n",
    "# Show result\n",
    "print(\"\\\\n Top 20 Customers by CLV:\")\n",
    "clv_summary.show(20, truncate=False)\n",
    "\n",
    "# Statistic Summary\n",
    "print(\"\\\\n CLV Statistic Summary.\")\n",
    "clv_summary.select(\n",
    "    _round(avg(\"clv\"), 2).alias(\"avg_clv\"),\n",
    "    _round(stddev(\"clv\"), 2).alias(\"stddev_clv\"),\n",
    "    _round(_min(\"clv\"), 2).alias(\"min_clv\"),\n",
    "    _round(_max(\"clv\"), 2).alias(\"max_clv\"),\n",
    "    _round(percentile_approx(\"clv\", 0.25), 2).alias(\"25th_percentile_clv\"),\n",
    "    _round(percentile_approx(\"clv\", 0.5), 2).alias(\"median_clv\"),\n",
    "    _round(percentile_approx(\"clv\", 0.75), 2).alias(\"75th_percentile_clv\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3. Identify The Top 1% of Customers Generating 80% of Revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n=== EXERCISE 3. Top 1% of Customers Generating 80% of Revenue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/30 10:36:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/30 10:36:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/30 10:36:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/30 10:36:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/30 10:36:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/30 10:36:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/30 10:36:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/30 10:36:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/30 10:36:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/30 10:36:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/30 10:36:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/30 10:36:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/30 10:36:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/30 10:36:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/30 10:36:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/30 10:36:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/30 10:36:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/30 10:36:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/30 10:36:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/30 10:36:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/30 10:36:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/30 10:36:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/30 10:36:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/30 10:36:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Revenue: $67416.51\n",
      "Total Customers: 599 customers\n",
      "1. Top 1% customers generate approximately 1.517269290% of total revenue.\n",
      "\\n== Top 1% Customers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/30 10:36:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/30 10:36:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/30 10:36:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/30 10:36:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/30 10:36:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/30 10:36:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/30 10:36:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/30 10:36:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/30 10:36:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/30 10:36:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/30 10:36:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/30 10:36:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+------+-----------+-----------+\n",
      "|customer_id|customer_name |clv   |total_trans|pct_revenue|\n",
      "+-----------+--------------+------+-----------+-----------+\n",
      "|526        |KARL SEAL     |221.55|45         |0.328628700|\n",
      "|148        |ELEANOR HUNT  |216.54|46         |0.649825980|\n",
      "|144        |CLARA SHAW    |195.58|42         |0.939932960|\n",
      "|137        |RHONDA KENNEDY|194.61|39         |1.228601120|\n",
      "|178        |MARION SNYDER |194.61|39         |1.517269290|\n",
      "+-----------+--------------+------+-----------+-----------+\n",
      "\n",
      "2. To generate 80% of revenue, approximately 73.12186978297161% of customers are required.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\\\n=== EXERCISE 3. Top 1% of Customers Generating 80% of Revenue\")\n",
    "\n",
    "# Compute total revenue\n",
    "total_revenue = clv_summary.agg(_sum(\"clv\")).first()[0]\n",
    "# Compute total customers\n",
    "total_customers = clv_summary.count()\n",
    "\n",
    "# Compute cumulative revenue\n",
    "windowSpec = Window.orderBy(col(\"clv\").desc())\n",
    "\n",
    "# Pareto\n",
    "## Ranking customer by clv\n",
    "## Cumulative revenue from customer #1 to present\n",
    "## % revenue at this present row\n",
    "## % number of customers at this present row\n",
    "customer_pareto = clv_summary \\\n",
    "    .withColumn(\"row_num\", row_number().over(windowSpec)) \\\n",
    "    .withColumn(\"cumulative_revenue\", _sum(\"clv\").over(windowSpec.rowsBetween(Window.unboundedPreceding, 0))) \\\n",
    "    .withColumn(\"pct_revenue\", col(\"cumulative_revenue\") / total_revenue * 100) \\\n",
    "    .withColumn(\"pct_customer\", col(\"row_num\") / total_customers * 100)\n",
    "\n",
    "# PART 1 — Top 1% customers generate how much revenue?\n",
    "top1pct_cutoff = int(total_customers * 0.01)\n",
    "\n",
    "top1pct_customer_revenue = customer_pareto \\\n",
    "    .filter(col(\"row_num\") <= top1pct_cutoff) \\\n",
    "    .agg(_max(\"pct_revenue\").alias(\"top1pct_customer_revenue\")) \\\n",
    "    .collect()[0][\"top1pct_customer_revenue\"]\n",
    "\n",
    "top1pct_customer = customer_pareto \\\n",
    "    .filter(col(\"row_num\") <= top1pct_cutoff) \\\n",
    "    .select(\"customer_id\", \"customer_name\", \"clv\", \"total_trans\", \"pct_revenue\")\n",
    "\n",
    "# PART 2 — How many customers are needed to reach 80% revenue?\n",
    "pct_customer_gen_80pct_revenue = customer_pareto \\\n",
    "    .filter(col(\"pct_revenue\") >= 80) \\\n",
    "    .agg(_min(\"pct_customer\").alias(\"pct_customer_gen_80pct_revenue\")) \\\n",
    "    .collect()[0][\"pct_customer_gen_80pct_revenue\"]\n",
    "\n",
    "\n",
    "# Summary Result\n",
    "print(f\"Total Revenue: ${total_revenue}\")\n",
    "print(f\"Total Customers: {total_customers} customers\")\n",
    "print(f\"1. Top 1% customers generate approximately {top1pct_customer_revenue}% of total revenue.\")\n",
    "print(\"\\\\n== Top 1% Customers:\")\n",
    "top1pct_customer.show(20, truncate=False)\n",
    "print(f\"2. To generate 80% of revenue, approximately {pct_customer_gen_80pct_revenue}% of customers are required.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4. Propose a partitioning strategy for the payment table:\n",
    "- by date?\n",
    "- by store?\n",
    "- by customer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\\\n=== EXERCISE 4. Partitioning Strategy Analysis\")\n",
    "\n",
    "# # ---- Strategy 1: By Date ----\n",
    "# print(\"\\n 1. PARTITIONING BY DATE (`payment_date`)\")\n",
    "# print(\"-\" * 60)\n",
    "\n",
    "# date_distribution = df_payment \\\n",
    "#     .withColumn(\"payment_month\", date_format(col(\"payment_date\"), \"yyyy-MM\")) \\\n",
    "#     .groupBy(\"payment_month\") \\\n",
    "#     .agg(\n",
    "#         count(\"payment_id\").alias(\"payment_count\"),\n",
    "#         _round(_sum(\"amount\"), 2).alias(\"total_revenue\")\n",
    "#     ) \\\n",
    "#     .orderBy(\"payment_month\")\n",
    "\n",
    "# date_distribution.show()\n",
    "\n",
    "# date_stats = date_distribution.agg(\n",
    "#     count(\"payment_month\").alias(\"total_months\"),\n",
    "#     _round(avg(\"payment_count\"), 0).alias(\"avg_payments_per_month\"),\n",
    "#     _round(stddev(\"payment_count\"), 0).alias(\"stddev_payments\"),\n",
    "#     _max(\"payment_count\").alias(\"max_payments\"),\n",
    "#     _min(\"payment_count\").alias(\"min_payments\")\n",
    "# )\n",
    "# date_stats.show()\n",
    "\n",
    "# # ---- Strategy 2: By Store ----\n",
    "# print(\"\\n 2. PARTITIONING BY STORE\")\n",
    "# print(\"-\" * 60)\n",
    "\n",
    "# store_distribution = df_payment \\\n",
    "#     .join(df_rental, \"rental_id\") \\\n",
    "#     .join(df_inventory, \"inventory_id\") \\\n",
    "#     .groupBy(\"store_id\") \\\n",
    "#     .agg(\n",
    "#         count(\"payment_id\").alias(\"payment_count\"),\n",
    "#         _round(_sum(\"amount\"), 2).alias(\"total_revenue\"),\n",
    "#         countDistinct(\"customer_id\").alias(\"unique_customers\")\n",
    "#     ) \\\n",
    "#     .orderBy(\"store_id\")\n",
    "\n",
    "# store_distribution.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5. Join Optimization Analysis\n",
    "\n",
    "The following join is very slow at scale:\n",
    "    \n",
    "`payment -> rental -> inventory -> film -> film_category -> category`\n",
    "\n",
    "Propose:\n",
    "- join order optimization\n",
    "- indexing strategies\n",
    "- caching or materialized views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n=== Exercise 5. Join Optimization Analysis\n",
      "\\n TABLE SIZE ANALYSIS:\n",
      "   payment        : 16,049 rows\n",
      "   rental         : 16,044 rows\n",
      "   inventory      : 4,581 rows\n",
      "   film           : 1,000 rows\n",
      "   film_category  : 1,000 rows\n",
      "   category       : 16 rows\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\\\n=== Exercise 5. Join Optimization Analysis\")\n",
    "print(\"\\\\n TABLE SIZE ANALYSIS:\")\n",
    "table_counts = {\n",
    "    \"payment\": df_payment.count(),\n",
    "    \"rental\": df_rental.count(),\n",
    "    \"inventory\": df_inventory.count(),\n",
    "    \"film\": df_film.count(),\n",
    "    \"film_category\": df_film_category.count(),\n",
    "    \"category\": df_category.count()\n",
    "}\n",
    "\n",
    "for table, count in table_counts.items():\n",
    "    print(f\"   {table:15s}: {count:,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Join Order Optimization\n",
    "\n",
    "*The goal of join order optimization is to **reduce the row count as early as possible**.*\n",
    "\n",
    "**Strategy:** Dimension Pre-Aggregation & Broadcast.\n",
    "\n",
    "In the current order, it is starting with `payment` and `rental`, which are high volume ~16k. If we are looking for a specific category such as *Action movies*, we are processing millions of payments only to filter them at the very last step.\n",
    "\n",
    "Therefore, to **OPTIMAL JOIN ORDER**:\n",
    "\n",
    "**Reverse the Join:**\n",
    "\n",
    "1. Start with smallest tables first:\n",
    "   `category` -> `film_category` -> `film`\n",
    "\n",
    "   - Film creates a small \"lookup\" set.\n",
    "   \n",
    "2. Then join to medium tables: -> `inventory`\n",
    "   \n",
    "3. Finally join to largest tables: -> `rental` -> `payment`\n",
    "\n",
    "Do: `category` -> `film_category` -> `film` -> `inventory` -> `rental` -> `payment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n=== 1. Join Order Optimization ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Join Order: 3.49 seconds (16,049 rows)\n",
      "Optimized Join Order: 2.20 seconds (16,049 rows)\n",
      "Improvement: 36.9% faster.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\\\n=== 1. Join Order Optimization ===\")\n",
    "start_bad = time.time()\n",
    "result_bad = df_payment \\\n",
    "    .join(df_rental, \"rental_id\") \\\n",
    "    .join(df_inventory, \"inventory_id\") \\\n",
    "    .join(df_film, \"film_id\") \\\n",
    "    .join(df_film_category, \"film_id\") \\\n",
    "    .join(df_category, \"category_id\") \\\n",
    "    .select(\"payment_id\", \"amount\", col(\"name\").alias(\"category_name\"))\n",
    "count_bad = result_bad.count()\n",
    "time_bad = time.time() - start_bad\n",
    "\n",
    "# Good join order (optimized)\n",
    "start_good = time.time()\n",
    "result_good = df_category \\\n",
    "    .join(df_film_category, \"category_id\") \\\n",
    "    .join(df_film, \"film_id\") \\\n",
    "    .join(df_inventory, \"film_id\") \\\n",
    "    .join(df_rental, \"inventory_id\") \\\n",
    "    .join(df_payment, \"rental_id\") \\\n",
    "    .select(\"payment_id\", \"amount\", col(\"name\").alias(\"category_name\"))\n",
    "count_good = result_good.count()\n",
    "time_good = time.time() - start_good\n",
    "\n",
    "print(f\"\"\"Original Join Order: {time_bad:.2f} seconds ({count_bad:,} rows)\n",
    "Optimized Join Order: {time_good:.2f} seconds ({count_good:,} rows)\n",
    "Improvement: {((time_bad - time_good) / time_bad * 100):.1f}% faster.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advanced:** Instead of a linear join, group the smaller \"Dimension\" tables (`category`, `film_category`, `film`, `inventory`) first. In Big Data frameworks like Spark, when the `category`, `film_category`, `film` and `inventory` tables are small enough to fit in memory, we use a **Broadcast Join**. This converts a distributed shuffle join into a local map-side join and avoids shuffling the massive `payment` and `rental` tables across the network.\n",
    "\n",
    "Proposed Order:\n",
    "\n",
    "1. Join `category` -> `film_category` -> `film` -> `inventory` \n",
    "\n",
    "    Result: `ref_inventory_dim` (Broadcasted).\n",
    "\n",
    "2. Join `payment` -> `rental`\n",
    "\n",
    "    Result: `fact_transactions`.\n",
    "\n",
    "3. Join `Fact_Transactions` and `Ref_Inventory_Dim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without Broadcast: 3.49 seconds\n",
      "With Broadcast: 1.81 seconds\n",
      "Improvement: 48.1% faster\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_broadcast = time.time()\n",
    "ref_inventory_dim = df_category \\\n",
    "    .join(df_film_category, \"category_id\") \\\n",
    "    .join(df_film, \"film_id\") \\\n",
    "    .join(df_inventory, \"film_id\")\n",
    "result_broadcast = df_payment \\\n",
    "    .join(df_rental, \"rental_id\") \\\n",
    "    .join(broadcast(ref_inventory_dim), \"inventory_id\") \\\n",
    "    .select(\"payment_id\", \"amount\", col(\"name\").alias(\"category_name\"))\n",
    "count_broadcast = result_broadcast.count()\n",
    "time_broadcast = time.time() - start_broadcast\n",
    "\n",
    "print(f\"\"\"Without Broadcast: {time_bad:.2f} seconds\n",
    "With Broadcast: {time_broadcast:.2f} seconds\n",
    "Improvement: {((time_bad - time_broadcast) / time_bad * 100):.1f}% faster\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Indexing Strategies\n",
    "\n",
    "A. For Relational Database PostgreSQL: Indexes should be added on all foreign keys involved in joins.\n",
    "\n",
    "B. For Distributed System Apache Spark: We can use partition method in Exercise 4 to store data in directories based on identified column. This enables **Partition Pruning**, allowing Spark to completely skip reading folders that do not match the query's time range.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Caching and Materialized Views\n",
    "\n",
    "To avoid re-computing this expensive 6-table join for every query, I propose implementing a multi-layer caching strategy.\n",
    "\n",
    "**Materialized Views**:\n",
    "\n",
    "- Denormalize the schema into a Flattened Fact Table. Pre-join all 6 tables into a single wide table: `dm_rental_analysis`.\n",
    "\n",
    "- Trade-off: Gain massive read speed but pay a cost in storage and refresh latency.\n",
    "\n",
    "**Distributed Caching**: \n",
    "\n",
    "- If the result of the `inventory` + `film` + `category` join is used across multiple downstream models, use `.persist(StorageLevel.MEMORY_AND_DISK)`.\n",
    "- Benefit: It stores the intermediate computed state in RAM, preventing the Spark DAG from re-evaluating the entire lineage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
